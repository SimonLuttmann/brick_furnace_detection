{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc643c9",
   "metadata": {
    "id": "9dc643c9"
   },
   "source": [
    "# Sentinel‑2 Kiln Detection – Baseline Notebook\n",
    "Ein modular aufgebautes Notebook zur Segmentierung kleiner Ziegelöfen in Sentinel‑2-Daten (Beispielland: Indien)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e4f74",
   "metadata": {
    "id": "516e4f74"
   },
   "source": [
    "## 1 · Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d779f10",
   "metadata": {
    "id": "4d779f10",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e012e4fe-7967-4659-ea03-4f589af3dce9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m22.2/22.2 MB\u001B[0m \u001B[31m67.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m118.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m92.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m51.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m6.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m87.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m962.6/962.6 kB\u001B[0m \u001B[31m61.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m825.4/825.4 kB\u001B[0m \u001B[31m56.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "# ─── 1 · Imports ──────────────────────────────────────────────────────────────\n",
    "import os, glob, random, zipfile, getpass, numpy as np, torch, rasterio, matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef0d3f",
   "metadata": {
    "id": "80ef0d3f"
   },
   "source": [
    "## 2 · Konstanten & Datenpfade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab3634",
   "metadata": {
    "id": "cdab3634"
   },
   "outputs": [],
   "source": [
    "# ─── 2 · Konstanten & Datenpfade – PALMA-ready ───────────────────────────────\n",
    "USER          = getpass.getuser()                     # z.B. 'tstraus2'\n",
    "SCRATCH       = os.environ.get(\"SLURM_TMPDIR\", f\"/scratch/tmp/{USER}\")\n",
    "ZIP_PATH      = f\"{SCRATCH}/Brick_Data_Train.zip\"     # hierher hast du die ZIP kopiert\n",
    "DATA_ROOT     = os.path.join(SCRATCH, \"Brick_Data_Train\")\n",
    "\n",
    "# Datensatz ggf. entpacken\n",
    "if not os.path.isdir(DATA_ROOT):\n",
    "    if os.path.isfile(ZIP_PATH):\n",
    "        print(f\"Entpacke {ZIP_PATH} nach {DATA_ROOT} …\")\n",
    "        with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "            zf.extractall(SCRATCH)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{ZIP_PATH} nicht gefunden!\")\n",
    "\n",
    "IMG_DIR      = os.path.join(DATA_ROOT, \"Image\")\n",
    "MASK_DIR     = os.path.join(DATA_ROOT, \"Mask\")        # Ordner-Name prüfen\n",
    "PATCH_SIZE   = 256\n",
    "BATCH_SIZE   = 8\n",
    "LR           = 1e-3\n",
    "EPOCHS       = 20\n",
    "NUM_CLASSES  = 10\n",
    "pl.seed_everything(42, workers=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a7144",
   "metadata": {
    "id": "3d2a7144"
   },
   "source": [
    "## 3 · Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "816fca46",
   "metadata": {
    "id": "816fca46"
   },
   "outputs": [],
   "source": [
    "# ─── 3 · Hilfsfunktion ───────────────────────────────────────────────────────\n",
    "def read_s2_image(path: str) -> torch.Tensor:\n",
    "    \"\"\"Liest Sentinel-2 GeoTIFF und skaliert DN auf [0,1].\"\"\"\n",
    "    with rasterio.open(path) as src:\n",
    "        img = src.read(out_dtype=np.float32)\n",
    "    return torch.from_numpy(img / 10000.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3417b",
   "metadata": {
    "id": "b1b3417b"
   },
   "source": [
    "### 3.1 · Dataset-Klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e94ff3d3",
   "metadata": {
    "id": "e94ff3d3"
   },
   "outputs": [],
   "source": [
    "# ─── 4 · Dataset-Klasse ──────────────────────────────────────────────────────\n",
    "class SentinelKilnDataset(Dataset):\n",
    "    \"\"\"Bild-/Masken-Dataset mit Zufalls-Patch und 10-Klassen-Label.\"\"\"\n",
    "    def __init__(self, img_dir, mask_dir, patch_size=256, transform=None):\n",
    "        self.img_paths  = sorted(glob.glob(os.path.join(img_dir, \"*.tif\")))\n",
    "        assert self.img_paths, f\"Keine Bilder in {img_dir}\"\n",
    "        self.mask_dir   = mask_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.norm       = transform or T.Normalize(mean=[0.3]*4, std=[0.2]*4)\n",
    "\n",
    "    def __len__(self): return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Bild\n",
    "        img_path = self.img_paths[idx]\n",
    "        img      = read_s2_image(img_path)[:4]\n",
    "\n",
    "        # Maske (0…10) → -1…9\n",
    "        mask_path = os.path.join(self.mask_dir, os.path.basename(img_path))\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = torch.from_numpy(src.read(1)).long() - 1   # -1 BG, 0…9 Klassen\n",
    "\n",
    "        # Zufälliger Patch\n",
    "        _, H, W = img.shape\n",
    "        top  = random.randint(0, H - self.patch_size)\n",
    "        left = random.randint(0, W - self.patch_size)\n",
    "        img  = img[:,  top:top+self.patch_size, left:left+self.patch_size]\n",
    "        mask = mask[   top:top+self.patch_size, left:left+self.patch_size]\n",
    "\n",
    "        img = self.norm(img)\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fa796",
   "metadata": {
    "id": "667fa796"
   },
   "source": [
    "### 3.2 · Daten-Vorschau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "644cab1e",
   "metadata": {
    "id": "644cab1e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "outputId": "55af28c1-cb15-40c0-e538-c753c6aa7bba"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAD9CAYAAABtAAQeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAE8BJREFUeJzt3XlwE+Xjx/HPpkmbSko5mtZyWLAizoAjgsoIFVEqCOVSEaXjiAcOjseo44B4zKh4yzHgCB6D0EGpOsWTKggKtCLifYGg6Hhx01LaQumRZH9/8CPfxhZbsbBPmvdrpn8k2aRPKJt39slm17Jt2xYAAHCUy+kBAAAAggwAgBEIMgAABiDIAAAYgCADAGAAggwAgAEIMgAABiDIAAAYgCADAGAAggwArdjatWtlWZaWLl3q9FDQBIJskLy8PFmWFf5xu93q3LmzrrvuOm3fvr3R+yxbtkyjRo1SWlqa4uPj1aFDBw0aNEizZs1SRUVFxLLdunWLeHyv16sePXpoypQp2rdv34l4ikDMqb9er1u3rsHttm2ra9eusixLI0eOdGCEMIXb6QGgoenTp6t79+6qrq7Whg0blJeXp3Xr1mnjxo3yer2SpFAopBtvvFF5eXk688wzdcstt6hr166qrKzUp59+qgceeEDvv/++Pvroo4jH7tOnj+6++25JUnV1tb766ivNmTNHRUVF+vzzz0/4cwVihdfrVX5+vrKysiKuLyoq0rZt25SQkODQyGAKgmyg4cOH65xzzpEkTZo0SSkpKXrqqaf07rvvavz48ZKkp59+Wnl5ebrrrrs0a9YsWZYVvv8dd9yhnTt3avHixQ0eu3PnzrrmmmvClydNmiSfz6eZM2dq69at6tGjx3F+dkBsGjFihAoKCvTMM8/I7f7fS29+fr769eunkpISB0cHEzBlHQUuuOACSdKvv/4qSaqqqtJTTz2lXr16acaMGRExPiI9PV333HNPsx7/5JNPlqSIFwkALWvChAkqLS3VqlWrwtfV1tZq6dKlys3NbbD8zJkzNWDAAHXs2FGJiYnq169fo58Dr1q1SllZWWrXrp18Pp969uyp++677x/HUlNTo5EjRyo5OVnr16+XdHjWbc6cOerVq5e8Xq/S0tI0efJklZWV/cdnjuYiyFHg999/lyS1b99ekrRu3Trt379fEyZMUFxc3L96rLq6OpWUlKikpETbtm3TsmXLNHv2bA0aNEjdu3dv6aED+H/dunXT+eefr1dffTV83fLly1VeXq6rr766wfJz587V2WefrenTp+vxxx+X2+3WlVdeqffeey+8zKZNmzRy5EjV1NRo+vTpmjVrlkaPHq1PPvnkqOM4dOiQRo0apfXr1+vDDz/UgAEDJEmTJ0/WlClTNHDgQM2dO1fXX3+9lixZomHDhqmurq4F/yVwNGwSGai8vFwlJSWqrq7WZ599pocfflgJCQnhHT62bNkiSerdu3fE/YLBYIN3sx07dozYgl65cqX8fn/EMgMHDtSbb755PJ4KgHpyc3N177336tChQ0pMTNSSJUt04YUXqlOnTg2W/fnnn5WYmBi+fNttt6lv376aPXu2cnJyJB3eOq6trdXy5cuVkpLS5O8/cOCARo4cqU2bNmn16tXq06ePpMNv8hcsWKAlS5ZEbK1fdNFFuvTSS1VQUNDoVjxaFlvIBsrOzpbf71fXrl01btw4tWnTRu+++666dOkiSeG9p30+X8T9fvjhB/n9/oif0tLSiGX69++vVatWadWqVSosLNRjjz2mTZs2afTo0Tp06NCJeYJAjBo/frwOHTqkwsJCVVZWqrCw8Kihqx/jsrIylZeX64ILLtDXX38dvr5du3aSpHfeeUehUOgff3d5ebmGDh2qLVu2aO3ateEYS1JBQYGSk5N1ySWXhGfQSkpK1K9fP/l8Pq1Zs+bYnzSajS1kA82bN0+nn366ysvLtXDhQhUXF0fsgZmUlCTp8Lvd+k477bTw51OLFy/Wyy+/3OCxU1JSlJ2dHb6ck5Ojnj17aty4cVqwYIFuv/324/GUAEjy+/3Kzs5Wfn6+qqqqFAwGNW7cuEaXLSws1KOPPqpvv/1WNTU14evrz3hdddVVWrBggSZNmqRp06ZpyJAhuvzyyzVu3Di5XJHbW3feeaeqq6v1zTffqFevXhG3bd26VeXl5UpNTW10LHv27DnWp4x/gSAb6LzzzgvvZT127FhlZWUpNzdXP/30k3w+n8444wxJ0saNGzVmzJjw/Xw+Xzi2jX3f8WiGDBkiSSouLibIwHGWm5urm266Sbt27dLw4cPDW7n1ffzxxxo9erQGDRqk+fPnKz09XR6PR4sWLVJ+fn54ucTERBUXF2vNmjV67733tGLFCr3++uu6+OKLtXLlyoh9TMaMGaPXXntNTz75pBYvXhwR7FAopNTUVC1ZsqTRMf/9Yy4cH0xZGy4uLk5PPPGEduzYoWeffVbS4b2uk5OT9dprrzU5TdUcgUBAUsMtbgAt77LLLpPL5dKGDRuOOl39xhtvyOv16oMPPtANN9yg4cOHR8xs1edyuTRkyBDNnj1bP/74ox577DGtXr26wTTz2LFjtXDhQuXn5+vWW2+NuC0zM1OlpaUaOHCgsrOzG/ycddZZLfPk8Y8IchQYPHiwzjvvPM2ZM0fV1dU66aSTNHXqVG3cuFHTpk2TbdsN7tPYdUezbNkySWKlA04An8+n5557Tg899JBGjRrV6DJxcXGyLEvBYDB83e+//6633347YrnGjrB35LPh+tPcR1x77bV65pln9Pzzz0d8LXL8+PEKBoN65JFHGtwnEAho//79zXhm+K+Yso4SU6ZM0ZVXXqm8vDzdfPPNmjZtmjZv3qwZM2Zo5cqVuuKKK9SlSxeVlZXp66+/VkFBgVJTU8NH9jpi+/bteuWVVyQd/g7kd999pxdeeEEpKSlMVwMnyMSJE//x9pycHM2ePVuXXnqpcnNztWfPHs2bN0+nnXaavv/++/By06dPV3FxsXJycpSRkaE9e/Zo/vz56tKlS4Mjgh1x2223qaKiQvfff7+Sk5N133336cILL9TkyZP1xBNP6Ntvv9XQoUPl8Xi0detWFRQUaO7cuUf9rBstyIYxFi1aZEuyv/jiiwa3BYNBOzMz087MzLQDgUD4+rfeesseMWKE7ff7bbfbbbdr187OysqyZ8yYYe/fvz/iMTIyMmxJ4R+Xy2WnpqbaEyZMsH/55Zfj/vyAWPRP63V9GRkZdk5OTvjySy+9ZPfo0cNOSEiwzzjjDHvRokX2gw8+aNd/2f7oo4/sMWPG2J06dbLj4+PtTp062RMmTLB//vnn8DJr1qyxJdkFBQURv2/q1Km2JPvZZ58NX/fiiy/a/fr1sxMTE+2kpCT7zDPPtKdOnWrv2LHjv/4zoBks2/4Xc5sAAOC44DNkAAAMQJABADAAQQYAwAAEGQAAAxBkAAAMQJABADAAQQYAwADNPlJX/TOMAPhvouXr/yav9y6XK+IUhS6XS2lpaTpw4IBKSkrCx2g3gcfj0SmnnCLLsrR9+3ZOddrKJSYmNvo3bmq959CZAKKOZVlKSUlRUlJSgzcNR44DbRoTx4SW5/P55Pf7VVFRodraWlVWVjb7vgQZQNSxLCsixrZta9euXQqFQgoEAqqrq3N4hI2LlpkRHLva2lqFQiG1b98+fHKQ5ka52YfO5N0d0HKi5YXZ5PW+/rl+JUWcGclEbdu2VWpqKlPWMSAuLk4ZGRlyuVwKhUIKhULatm1bk28U2akLQFQKBoMRP6Y7cu5yt5uJydYuGAzqjz/+UE1NjUKhULP/5vzPAIATKDU1VVVVVVHxJgLHLhgM6q+//lKbNm3k8/maNStGkAHgBCovLw9vLaP1O3jwoA4ePNisZfkMGXAAnyHHHpfLpfj4+PBOP4g9Ta33BBlwAEEGYk9T6z07dQEAYACCDACAAQgyAAAGYC9rAGhl0tPTFRcXp6qqKu3bt8/p4aCZCDIARDGXyyWPx6POnTuHrzuyM15NTY1Tw8IxIMgAEMX8fr/atGnTIL51dXXau3evQ6PCseBrT4AD+NoTWlL79u1VVlbm9DDQBL6HDBiIIAOxh+8hAwAQBQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGcDs9AABA7MrOzlZSUpIkacuWLdq8ebPDI3IOQQYAOOKiiy5Sz5495XIdnqzdvXu3LMuKmvOFtzSmrAEAjoiPjw/H2LZt9e3bV926dXN2UA4iyAAAx1mWpdLSUh04cMDpoTiGIAMAHLF582YVFRUpFAqptLRUq1ev1t69e50elmMsu5mT9ZZlHe+xADEjWj4jY73HiZCWlqba2lqVlZU5PZTjqqn1niADDiDIQOxpar1nyhoAAAMQZAAADECQAQAwAEEGAMAABBkAAAMQZAAADECQAQAwAEEGAMAABBkAAAMQZAAADECQAQAwAEEGAMAABBkAAAMQZABwUIcOHZSQkOD0MGAATr8IOIDTL8YWj8ejU0455ai3BwIB/fnnn1Hz/wLHpqm/r/sEjQMAYlYoFNLBgwePentpaSkxBlvIgBOi5cWX9R5oOU2t93yGDACAAZiyBoAY4PV6NX/+fHk8HgUCAYVCIUnS1q1b9eSTTzo8OkhMWQOOYMoaTsjIyNC0adOUn5+vjz/+2OnhxJym1nuCDDiAIAOxh8+QAQCIAgQZAAADEGQAAAxAkAEAMABBBgDAAAQZAAADEGQAAAxAkAEAMACHzgSAVig9PV2SVFFR8Y9nmoI5CDIAtDK9e/fW4MGDZVmWfv31VxUVFRHlKMCUNQC0In379lVWVlb4sKeZmZlq27atw6NCcxBkAGhFunTpIrfbHTXHS8f/EGQAaEUKCwtVVVUV3kKura0Nn2oRZuMzZABoRUKhkLZv3y6v1ytJ+uGHH7R7926HR4Xm4PSLgAOiZTqR9R5oOZx+EQCAKECQAQAwAEEGAMAABBkAAAMQZAAADMDXngDgbyzL0sknnxy+XFJSorq6OgdHhFhAkAGgESeddJKkw3FOSEhQMBjUX3/95fCo0JrxPWTAAXwPOTr4fD516NBBkuTxeGTbtsrKylRWVubwyBCNmlrvCTLgAIIcfVJTUxUIBLRv3z6nh4IoRZABAxFkIPZwpC4AAKIAQQYAhzETAYkgA4CjUlJSdOqpp4bPzoTYRZABwGGWZSk9Pd3pYcBhBBkADMDe2yDIAGCAyspKp4cAh3GkLgBwkJM7dCUkJGjixInhy7Zta+HChQoGg46NKZaxhQwADklKSlLbtm0d+/1+v18JCQkRP36/37HxxDqCDAAOqa2tVUVFhcrLyx05WExmZmbEZcuyNGLEiBM+DhxGkAHAITU1Ndq7d6/27t3rSJA3bNgQcdm2ba1fv/6EjwOHEWQAiFG1tbX68MMPZdu2bNvWihUrtGXLFqeHFbM4ljXgAI5lDVNYlqW4uDhJUiAQcHg0rVtT6z17WQNADLNtmxAbgilrAAAMQJABADAAQQYAwAAEGQAAAxBkAAAMQJABwGBut1uJiYlODwMnAEEGAIN5PB75/X55vV6nh4LjjCADgOHi4+PldnPYiNaOIANAFEhLS5PH43F6GDiOCDIAAAYgyABgsLq6Oh08eJDjiscATi4BOICTS+DfcLvd8ng8qq6ujpr/O2ioqb8dQQYcEC0vqqz3QMtpar1nyhoAAAMQZAAADECQAQAwAEEGAMAABBkAAAMQZAAADECQAQAwAEEGAMAABBkAAAMQZAAADECQAQAwAEEGAMAABBkAAAMQZAAADOB2egAAYBrLspSWlha+XFpaqrq6OgdHhFjAFjIA/M2wYcO0Y8cO7dy5Uzt37lSfPn2cHhJiAEEGgL955ZVXZFlW+PK1116r+Ph4B0eEWGDZtm03a8F6/zkB/DfNXO0cF6vrfUlJiTp27Bi+bNu2kpOTVVlZ6eCoEO2aWu/ZQgaAvxk8eLCCwaAkKRQK6YorrtCBAwccHhVaO4IMAH+zceNGnXvuufrtt980ceJEvf3221Ezq4HoxZQ14IBoeXFnvQdaDlPWAABEAYIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAAAYgyAAAGIAgAwBgAIIMAIABCDIAoFVp3769hg0bpvT0dKeH8q+4nR4AAADHYtKkSfJ6vQ2u79Chg/r376+ZM2dq586dDozs2BBkAEDUuf322zV06FDFxcXJtu0GtxcXF+vLL790YGTHzrIbeyaNLWhZx3ssQMxo5mrnONZ7mCohIUHz58+Xz+dTUVGRFi1aFHF7IBBQXV2dQ6NrXFPrPUEGHECQgdjT1HrPTl0AABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABjAsm3bdnoQAADEOraQAQAwAEEGAMAABBkAAAMQZAAADECQAQAwAEEGAMAABBkAAAMQZAAADECQAQAwwP8BvZ21GUnyyoAAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ─── 5 · Daten-Vorschau ──────────────────────────────────────────────────────\n",
    "ds_preview = SentinelKilnDataset(IMG_DIR, MASK_DIR, PATCH_SIZE)\n",
    "x, y = ds_preview[0]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1); plt.title(\"RGB\"); plt.imshow((x[[2,1,0]]*2.5).clamp(0,1).permute(1,2,0)); plt.axis(\"off\")\n",
    "plt.subplot(1,2,2); plt.title(\"Maske\"); plt.imshow(y, cmap=\"tab10\"); plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8f94d",
   "metadata": {
    "id": "17b8f94d"
   },
   "source": [
    "### 3.3 · DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b5a3385",
   "metadata": {
    "id": "0b5a3385",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5219aa64-3c19-45f5-98a8-80a45883c25e"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(841, 94)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# ─── 6 · DataLoader ──────────────────────────────────────────────────────────\n",
    "def build_loaders():\n",
    "    ds = SentinelKilnDataset(IMG_DIR, MASK_DIR, PATCH_SIZE)\n",
    "    val_len = max(1, int(0.1 * len(ds)))\n",
    "    train_ds, val_ds = random_split(ds, [len(ds) - val_len, val_len])\n",
    "    dl_kwargs = dict(batch_size=BATCH_SIZE,\n",
    "                     num_workers=os.cpu_count(),\n",
    "                     pin_memory=torch.cuda.is_available())\n",
    "    return (DataLoader(train_ds, shuffle=True,  **dl_kwargs),\n",
    "            DataLoader(val_ds,   shuffle=False, **dl_kwargs))\n",
    "\n",
    "train_loader, val_loader = build_loaders()\n",
    "print(len(train_loader), \"Train-Batches |\", len(val_loader), \"Val-Batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d767b68",
   "metadata": {
    "id": "5d767b68"
   },
   "source": [
    "## 4 · Modell – minimaler UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd9acbf",
   "metadata": {
    "id": "2bd9acbf"
   },
   "outputs": [],
   "source": [
    "# ─── 7 · Modell – UNet (10 Klassen) ──────────────────────────────────────────\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, inc, outc):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(inc, outc, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outc), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outc, outc, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outc), nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class UNet(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.enc1 = DoubleConv(4,64); self.enc2 = DoubleConv(64,128)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up   = nn.ConvTranspose2d(128,64,2,2)\n",
    "        self.dec1 = DoubleConv(128,64)\n",
    "        self.out  = nn.Conv2d(64, num_classes, 1)\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        c1 = self.enc1(x); x = self.pool(c1); x = self.enc2(x)\n",
    "        x  = self.up(x);  x = torch.cat([x,c1],1); x = self.dec1(x)\n",
    "        return self.out(x)\n",
    "\n",
    "    def _step(self,b):\n",
    "        x,y = b; logit = self(x); return self.loss(logit,y)\n",
    "\n",
    "    def training_step(self,b,idx):\n",
    "        l = self._step(b); self.log(\"train_loss\",l,prog_bar=True); return l\n",
    "    def validation_step(self,b,idx):\n",
    "        l = self._step(b); self.log(\"val_loss\",l,prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13819d4",
   "metadata": {
    "id": "b13819d4"
   },
   "source": [
    "## 5 · Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ab6c1",
   "metadata": {
    "id": "a29ab6c1"
   },
   "outputs": [],
   "source": [
    "# ─── 8 · Training ────────────────────────────────────────────────────────────\n",
    "model  = UNet(lr=LR)\n",
    "ckpt   = ModelCheckpoint(monitor=\"val_loss\", save_top_k=1)\n",
    "lrmon  = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "trainer = pl.Trainer(accelerator=\"auto\", devices=1,\n",
    "                     precision=\"16-mixed\", max_epochs=EPOCHS,\n",
    "                     callbacks=[ckpt, lrmon], log_every_n_steps=10)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba95934",
   "metadata": {
    "id": "9ba95934"
   },
   "source": [
    "## 6 · Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11926897",
   "metadata": {
    "id": "11926897"
   },
   "outputs": [],
   "source": [
    "# ─── 9 · Evaluation ──────────────────────────────────────────────────────────\n",
    "import torchmetrics\n",
    "\n",
    "metric = torchmetrics.classification.MulticlassJaccardIndex(\n",
    "    num_classes=NUM_CLASSES, ignore_index=-1, average='none'\n",
    ").to(model.device)\n",
    "\n",
    "model.eval(); metric.reset()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        preds = torch.argmax(model(xb.to(model.device)), 1)\n",
    "        metric.update(preds.cpu(), yb)\n",
    "\n",
    "ious  = metric.compute()\n",
    "valid = ~torch.isnan(ious)\n",
    "print(\"IoU pro Klasse:\", ious)\n",
    "print(\"Mean IoU:\", ious[valid].mean().item() if valid.any() else 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516f562",
   "metadata": {
    "id": "5516f562"
   },
   "source": [
    "## 7 · Inference & Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798a5ef",
   "metadata": {
    "id": "a798a5ef"
   },
   "outputs": [],
   "source": [
    "# ─── 10 · Inference & Visualisierung ─────────────────────────────────────────\n",
    "def infer_full_tif(path_img, norm_fn, window=PATCH_SIZE):\n",
    "    img = read_s2_image(path_img)[:4]\n",
    "    img = norm_fn(img)                         # Szenen-Normierung\n",
    "    _, H, W = img.shape\n",
    "    full_logits = torch.zeros(NUM_CLASSES, H, W)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for top in range(0, H-window+1, window):\n",
    "            for left in range(0, W-window+1, window):\n",
    "                patch = img[:, top:top+window, left:left+window].unsqueeze(0).to(model.device)\n",
    "                full_logits[:, top:top+window, left:left+window] = model(patch).cpu().squeeze(0)\n",
    "\n",
    "    pred = full_logits.argmax(0)               # [H,W] Klassenindex\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(121); plt.title(\"RGB\")\n",
    "    plt.imshow((img[[2,1,0]]*2.5).clamp(0,1).permute(1,2,0)); plt.axis(\"off\")\n",
    "    plt.subplot(122); plt.title(\"Prediction\")\n",
    "    plt.imshow(pred, cmap=\"tab10\"); plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4f551",
   "metadata": {
    "id": "0bd4f551"
   },
   "source": [
    "## 8 · Gewichts‑Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a102ed3",
   "metadata": {
    "id": "6a102ed3"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'unet_kiln_sentinel2.pt')\n",
    "print('Model gespeichert.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
